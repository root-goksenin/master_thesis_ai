#!/bin/bash

#SBATCH --partition=gpu
#SBATCH --gpus=1
#SBATCH --job-name=fiqa-eval
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=18
#SBATCH --time=01:30:00
#SBATCH --output=slurm_output_%A.out

module load 2022
module load Anaconda3/2022.05 
source activate gpl_env_1

cd $HOME/master_thesis_ai
export CUBLAS_WORKSPACE_CONFIG=:4096:8
export LC_ALL=C.UTF-8
export LANG=C.UTF-8

# Evaluate trained models from us
data_name=fiqa
bm25_weights=(2.0 1.0 0.6 0.1)
aug_strategy=(no_aug)
model_name=("mini_lm" "tiny_bert" "two_teacher_average" "three_teacher_average")

# Eval bm25
python3 eval.py --data_name ${data_name} --eval_bm25
# Eval baseline for data
python3 eval.py --data_name ${data_name} 

# Eval baseline with bm25 weights
# Loop through bm25_weights
for bm25_weight in ${bm25_weights[@]}; do
  python3 eval.py --data_name ${data_name} --use_bm25 --bm25_weight=${bm25_weight}
done

# Evaluate GPL trained models
# Loop through model_name and aug_strategy
for model in "${model_name[@]}"; do
  srun python eval.py --data_name ${data_name} --aug_strategy ${aug_strategy} --model_name ${model} --load_pretrained
done

# Evaluate trained models with bm25 weights
# Loop through model_name, aug_strategy, and bm25_weights
for model in "${model_name[@]}"; do
  for bm25_weight in ${bm25_weights[@]}; do
    srun python eval.py --data_name ${data_name} --aug_strategy ${aug_strategy} --model_name ${model} --load_pretrained --use_bm25 --bm25_weight=${bm25_weight}
  done
done

# Evaluate tas-b model
python3 eval.py --data_name ${data_name} --eval_tasb_gpl

# Evaluate tas-b model with bm25 weights
# Loop through bm25_weights
for bm25_weight in ${bm25_weights[@]}; do
  python3 eval.py --data_name ${data_name} --eval_tasb_gpl --use_bm25 --bm25_weight=${bm25_weight}
done


